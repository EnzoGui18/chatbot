{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Cv5SDixlSf31rGAmEosyBKggz38fgbX_",
      "authorship_tag": "ABX9TyPPDdTWMEZxY+6yAH/leyIF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "965b9b7648874f998c112c7ff441d61c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_d347bca6b4f6468b92d00f70fd69dba6"
          }
        },
        "600b1493f4e849c0b716291188c2fe8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b924d7957a040c99c56ec8e021f70f3",
            "placeholder": "​",
            "style": "IPY_MODEL_bc8b0642734f44878e66014e293caa84",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "461e262df709423a9c22cac7e7b5ddb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_4b0423bed5e04dcdadb5c605fbcc8904",
            "placeholder": "​",
            "style": "IPY_MODEL_2d9901757c914c98b7aaa40fba890429",
            "value": ""
          }
        },
        "386bef3a75c54815a041cbdc6a1e8fd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_806dfbb4d15a4393b1f944cf41fba6fc",
            "style": "IPY_MODEL_aeac5fd20d8a4a48b6400e4577fb63d4",
            "value": true
          }
        },
        "511100f37b204a7ab8bb100f2fd2115c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_8800840705584d8d91bf215882c2af6d",
            "style": "IPY_MODEL_6eb8a7b1ade1474285204d92678f7fe3",
            "tooltip": ""
          }
        },
        "d4033a08bad147178998b517bf090c17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12d4c482f7ba4e809db7fc0e5e906a09",
            "placeholder": "​",
            "style": "IPY_MODEL_2023479ea3ef49409e345f4a990bb9db",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "d347bca6b4f6468b92d00f70fd69dba6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "0b924d7957a040c99c56ec8e021f70f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc8b0642734f44878e66014e293caa84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b0423bed5e04dcdadb5c605fbcc8904": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d9901757c914c98b7aaa40fba890429": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "806dfbb4d15a4393b1f944cf41fba6fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aeac5fd20d8a4a48b6400e4577fb63d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8800840705584d8d91bf215882c2af6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6eb8a7b1ade1474285204d92678f7fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "12d4c482f7ba4e809db7fc0e5e906a09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2023479ea3ef49409e345f4a990bb9db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b8f6b6516244d3ebb1ed5e16608d608": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51479be971854d358514aaa50d37aec0",
            "placeholder": "​",
            "style": "IPY_MODEL_7dff2b8d1d0247ea849efda1153d3bd0",
            "value": "Connecting..."
          }
        },
        "51479be971854d358514aaa50d37aec0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dff2b8d1d0247ea849efda1153d3bd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5019b071b19f4eeebee6a23b9abdedbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db4e614a44d64c2196331e94594c73e9",
              "IPY_MODEL_3bae76a5478343a392553a3b6a695437",
              "IPY_MODEL_b37637ff468d4248ac03db476095b7bc"
            ],
            "layout": "IPY_MODEL_687ec20ca6e14763b738d8cc06e26084"
          }
        },
        "db4e614a44d64c2196331e94594c73e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_727d4b1148444a85b35ff8c840ba2792",
            "placeholder": "​",
            "style": "IPY_MODEL_e17844ff97954765892546f3bbbb50dd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3bae76a5478343a392553a3b6a695437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36d84f45bd6a43659b2594a9c40c2cd3",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9502c795426444799d13bcab557aea01",
            "value": 4
          }
        },
        "b37637ff468d4248ac03db476095b7bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcd5cbf7a62a4111a1639e93c545d3e3",
            "placeholder": "​",
            "style": "IPY_MODEL_657f69c6377f4883a0fed19dbe9f04df",
            "value": " 4/4 [01:21&lt;00:00, 17.17s/it]"
          }
        },
        "687ec20ca6e14763b738d8cc06e26084": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "727d4b1148444a85b35ff8c840ba2792": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e17844ff97954765892546f3bbbb50dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36d84f45bd6a43659b2594a9c40c2cd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9502c795426444799d13bcab557aea01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fcd5cbf7a62a4111a1639e93c545d3e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "657f69c6377f4883a0fed19dbe9f04df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EnzoGui18/chatbot/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8P131sNzwep",
        "outputId": "3653d744-5f54-48ac-b38f-c9fbc70d03c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Ambiente completo para execução local no Colab instalado!\n"
          ]
        }
      ],
      "source": [
        "!pip install -q \\\n",
        "    transformers \\\n",
        "    torch \\\n",
        "    accelerate \\\n",
        "    bitsandbytes \\\n",
        "    sentence-transformers \\\n",
        "    faiss-cpu \\\n",
        "    gradio \\\n",
        "    pdfplumber \\\n",
        "    beautifulsoup4 \\\n",
        "    requests \\\n",
        "    langchain \\\n",
        "    langchain-community \\\n",
        "    langchain-text-splitters\n",
        "\n",
        "print(\"✅ Ambiente completo para execução local no Colab instalado!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Cole seu token de acesso do Hugging Face\n",
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "965b9b7648874f998c112c7ff441d61c",
            "600b1493f4e849c0b716291188c2fe8f",
            "461e262df709423a9c22cac7e7b5ddb8",
            "386bef3a75c54815a041cbdc6a1e8fd1",
            "511100f37b204a7ab8bb100f2fd2115c",
            "d4033a08bad147178998b517bf090c17",
            "d347bca6b4f6468b92d00f70fd69dba6",
            "0b924d7957a040c99c56ec8e021f70f3",
            "bc8b0642734f44878e66014e293caa84",
            "4b0423bed5e04dcdadb5c605fbcc8904",
            "2d9901757c914c98b7aaa40fba890429",
            "806dfbb4d15a4393b1f944cf41fba6fc",
            "aeac5fd20d8a4a48b6400e4577fb63d4",
            "8800840705584d8d91bf215882c2af6d",
            "6eb8a7b1ade1474285204d92678f7fe3",
            "12d4c482f7ba4e809db7fc0e5e906a09",
            "2023479ea3ef49409e345f4a990bb9db",
            "6b8f6b6516244d3ebb1ed5e16608d608",
            "51479be971854d358514aaa50d37aec0",
            "7dff2b8d1d0247ea849efda1153d3bd0"
          ]
        },
        "id": "0NR7Tu-o1UNh",
        "outputId": "4c2b3c23-66e7-4c7b-e18e-dba110e0ced1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "965b9b7648874f998c112c7ff441d61c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Define o dispositivo como a GPU do Colab\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "# --- 1. Carregar o Modelo de Embedding ---\n",
        "print(\"\\nCarregando modelo de embedding 'Legal-BERTimbau-large'...\")\n",
        "embedding_model = SentenceTransformer('rufimelo/Legal-BERTimbau-large', device=device)\n",
        "print(\"✅ Modelo de Embedding carregado.\")\n",
        "\n",
        "# --- 2. Carregar o Modelo de Linguagem (LLM) com Quantização ---\n",
        "LLM_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# Configuração de quantização correta\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "print(f\"\\nCarregando LLM '{LLM_MODEL_NAME}' (pode demorar)...\")\n",
        "llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "    LLM_MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "print(\"✅ LLM carregado com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347,
          "referenced_widgets": [
            "5019b071b19f4eeebee6a23b9abdedbf",
            "db4e614a44d64c2196331e94594c73e9",
            "3bae76a5478343a392553a3b6a695437",
            "b37637ff468d4248ac03db476095b7bc",
            "687ec20ca6e14763b738d8cc06e26084",
            "727d4b1148444a85b35ff8c840ba2792",
            "e17844ff97954765892546f3bbbb50dd",
            "36d84f45bd6a43659b2594a9c40c2cd3",
            "9502c795426444799d13bcab557aea01",
            "fcd5cbf7a62a4111a1639e93c545d3e3",
            "657f69c6377f4883a0fed19dbe9f04df"
          ]
        },
        "id": "poVPfYd81eAq",
        "outputId": "0f0d6289-551c-4e91-edec-10b8e146b8f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando dispositivo: cuda\n",
            "\n",
            "Carregando modelo de embedding 'Legal-BERTimbau-large'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name rufimelo/Legal-BERTimbau-large. Creating a new one with mean pooling.\n",
            "Some weights of BertModel were not initialized from the model checkpoint at rufimelo/Legal-BERTimbau-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Modelo de Embedding carregado.\n",
            "\n",
            "Carregando LLM 'meta-llama/Meta-Llama-3-8B-Instruct' (pode demorar)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5019b071b19f4eeebee6a23b9abdedbf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LLM carregado com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "import pdfplumber\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "class PDFReader:\n",
        "    def __init__(self, data_dir: str = \"data\"):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self._create_directory()\n",
        "    def _create_directory(self) -> None:\n",
        "        if not self.data_dir.exists(): self.data_dir.mkdir(parents=True, exist_ok=True)\n",
        "    def _find_pdfs(self) -> List[Path]:\n",
        "        return list(self.data_dir.glob(\"*.pdf\"))\n",
        "    def _extract_text(self, pdf_path: Path) -> str:\n",
        "        text = \"\"\n",
        "        try:\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                for page in pdf.pages:\n",
        "                    page_text = page.extract_text(); text += page_text + \"\\n\" if page_text else \"\"\n",
        "        except Exception as e: print(f\"❌ Erro ao processar {pdf_path.name}: {e}\")\n",
        "        return text\n",
        "    def read_all_pdfs(self) -> Dict[str, str]:\n",
        "        pdf_files = self._find_pdfs()\n",
        "        if not pdf_files: print(f\"❌ Nenhum PDF encontrado em '{self.data_dir}'\"); return {}\n",
        "        documents = {}; print(f\"🔎 Processando {len(pdf_files)} arquivo(s) PDF...\")\n",
        "        for pdf_path in pdf_files:\n",
        "            text = self._extract_text(pdf_path)\n",
        "            if text.strip(): documents[pdf_path.name] = text; print(f\"📄✔ {pdf_path.name}\")\n",
        "        return documents\n",
        "\n",
        "class DocumentLoader:\n",
        "    def __init__(self, data_dir: str = \"data\"):\n",
        "        self.pdf_reader = PDFReader(data_dir); self.documents = {}\n",
        "    def load_pdfs(self):\n",
        "        print(\"\\n📄 Carregando PDFs...\"); pdf_docs = self.pdf_reader.read_all_pdfs(); self.documents.update(pdf_docs)\n",
        "    def load_all(self):\n",
        "        self.load_pdfs()\n",
        "        print(\"\\n\" + \"=\"*30 + \"\\n📊 Resumo do Carregamento:\")\n",
        "        print(f\"  - Total de documentos: {len(self.documents)}\")\n",
        "        print(f\"  - Total de caracteres: {sum(len(c) for c in self.documents.values())}\")\n",
        "        print(\"=\"*30)\n",
        "\n",
        "print(\"✅ Classes PDFReader e DocumentLoader definidas.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWzb-4SY42_i",
        "outputId": "99b181a0-9d9c-4a98-82d5-341ada5d2a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Classes PDFReader e DocumentLoader definidas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text); text = re.sub(r'\\s{2,}', ' ', text)\n",
        "    lines = text.split('\\n'); clean_lines = [line.strip() for line in lines if len(line.strip()) > 10]\n",
        "    return \"\\n\".join(clean_lines)\n",
        "\n",
        "def create_chunks(documents: Dict[str, str], chunk_size=1000, chunk_overlap=200) -> Dict[str, List[str]]:\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    chunked_docs = {}; print(\"\\n쪼 Dividindo documentos em chunks...\")\n",
        "    for doc_name, content in documents.items():\n",
        "        chunks = text_splitter.split_text(content)\n",
        "        chunked_docs[doc_name] = chunks\n",
        "        print(f\"  - '{doc_name}' dividido em {len(chunks)} chunks.\")\n",
        "    return chunked_docs\n",
        "\n",
        "print(\"✅ Funções clean_text e create_chunks definidas.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-QNjYpt45qI",
        "outputId": "376b9262-8b33-458a-d023-e943fe5fd929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Funções clean_text e create_chunks definidas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Carregar os documentos\n",
        "loader = DocumentLoader()\n",
        "loader.load_all()\n",
        "raw_documents = loader.documents\n",
        "\n",
        "# 2. Limpar os textos\n",
        "cleaned_documents = {doc_name: clean_text(content) for doc_name, content in raw_documents.items()}\n",
        "print(\"\\n✨ Textos limpos com sucesso!\")\n",
        "\n",
        "# 3. Dividir em Chunks\n",
        "chunked_documents = create_chunks(cleaned_documents)\n",
        "print(\"\\n✅ Pipeline de preparação de dados concluído!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZyRVY8y6d7x",
        "outputId": "93db2f24-bb7c-40fd-9c4a-211f3592f0b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📄 Carregando PDFs...\n",
            "🔎 Processando 3 arquivo(s) PDF...\n",
            "📄✔ Consumidor.pdf\n",
            "📄✔ civil.pdf\n",
            "📄✔ trabalhista.pdf\n",
            "\n",
            "==============================\n",
            "📊 Resumo do Carregamento:\n",
            "  - Total de documentos: 3\n",
            "  - Total de caracteres: 2125765\n",
            "==============================\n",
            "\n",
            "✨ Textos limpos com sucesso!\n",
            "\n",
            "쪼 Dividindo documentos em chunks...\n",
            "  - 'Consumidor.pdf' dividido em 104 chunks.\n",
            "  - 'civil.pdf' dividido em 822 chunks.\n",
            "  - 'trabalhista.pdf' dividido em 1721 chunks.\n",
            "\n",
            "✅ Pipeline de preparação de dados concluído!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Célula 8 (Corrigida e Completa)\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings # <-- IMPORTANTE: A importação do nosso \"adaptador\"\n",
        "\n",
        "# O FAISS prefere uma lista de objetos 'Document' do LangChain.\n",
        "# Esta parte do código já está correta.\n",
        "documents_for_faiss = []\n",
        "for doc_name, chunks in chunked_documents.items():\n",
        "    for chunk_text in chunks:\n",
        "        documents_for_faiss.append(\n",
        "            Document(page_content=chunk_text, metadata={\"source\": doc_name})\n",
        "        )\n",
        "\n",
        "print(f\"\\nTotal de {len(documents_for_faiss)} chunks preparados para vetorização.\")\n",
        "\n",
        "# --- A CORREÇÃO ESTÁ AQUI ---\n",
        "# 1. Criamos o \"adaptador\" compatível com o LangChain.\n",
        "# Em vez de passar nosso 'embedding_model' diretamente, nós instanciamos\n",
        "# a classe HuggingFaceEmbeddings, especificando o nome do modelo.\n",
        "langchain_embedder = HuggingFaceEmbeddings(\n",
        "    model_name='rufimelo/Legal-BERTimbau-large',\n",
        "    model_kwargs={'device': device} # Garante que ele use a GPU que já definimos\n",
        ")\n",
        "print(\"Adaptador de embedding para LangChain criado.\")\n",
        "\n",
        "print(\"\\nVetorizando os documentos e criando o índice FAISS...\")\n",
        "\n",
        "# 2. Usamos o novo 'langchain_embedder' para criar o vector_store\n",
        "vector_store = FAISS.from_documents(documents_for_faiss, langchain_embedder)\n",
        "\n",
        "print(\"✅ Banco de dados vetorial FAISS criado com sucesso!\")"
      ],
      "metadata": {
        "id": "5MdZ1d2N7NPf",
        "outputId": "6a874843-3544-40fb-9661-319a48f512a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total de 2647 chunks preparados para vetorização.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-715655568.py:22: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  langchain_embedder = HuggingFaceEmbeddings(\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name rufimelo/Legal-BERTimbau-large. Creating a new one with mean pooling.\n",
            "Some weights of BertModel were not initialized from the model checkpoint at rufimelo/Legal-BERTimbau-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adaptador de embedding para LangChain criado.\n",
            "\n",
            "Vetorizando os documentos e criando o índice FAISS...\n",
            "✅ Banco de dados vetorial FAISS criado com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "print(\"Preparando o LLM para integração com o LangChain...\")\n",
        "\n",
        "# Cria um pipeline de geração de texto do Transformers\n",
        "# Este é o invólucro padrão para usar modelos como o Llama 3\n",
        "text_generation_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=llm_model,\n",
        "    tokenizer=llm_tokenizer,\n",
        "    max_new_tokens=1024, # Define o tamanho máximo da resposta\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# \"Embrulha\" o pipeline na classe compatível com LangChain\n",
        "llm_for_chain = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "\n",
        "print(\"✅ LLM pronto para ser usado na cadeia RAG!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKgmhkQJKsFB",
        "outputId": "625ad73a-4913-4b02-aee6-8b7d4202dacd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparando o LLM para integração com o LangChain...\n",
            "✅ LLM pronto para ser usado na cadeia RAG!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3970723560.py:17: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm_for_chain = HuggingFacePipeline(pipeline=text_generation_pipeline)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "print(\"Construindo a cadeia RAG...\")\n",
        "\n",
        "# --- 1. Definir o Retriever a partir do nosso Vector Store ---\n",
        "\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5}) # k=3 significa que buscará os 3 chunks mais relevantes\n",
        "# --- 2. Criar o Template do Prompt ---\n",
        "\n",
        "template = \"\"\"\n",
        "Você é um assistente jurídico no Brasil. Use APENAS o contexto a seguir para responder à pergunta.\n",
        "Se a resposta não estiver no contexto, diga \"Com base nos documentos fornecidos, não encontrei a informação.\"\n",
        "Cite a fonte do documento de onde extraiu a resposta.\n",
        "\n",
        "Contexto: {context}\n",
        "\n",
        "Pergunta: {question}\n",
        "\n",
        "---\n",
        "Após a resposta, adicione em uma nova linha sua confiança na resposta de 0 a 100% no formato \"Confiança: XX%\". A confiança deve ser alta se o contexto responde diretamente à pergunta.\n",
        "\n",
        "Resposta:\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# --- 3. Função para formatar os documentos recuperados ---\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(f\"Fonte: {doc.metadata.get('source', 'N/A')}\\n{doc.page_content}\" for doc in docs)\n",
        "\n",
        "# --- 4. Montar a Cadeia (Chain) ---\n",
        "# Esta é a sequência de passos que o sistema executará\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm_for_chain\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"✅ Cadeia RAG montada e pronta para uso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwZoIMPtKxAi",
        "outputId": "7df13b5f-8937-422c-fc60-3775c2a121b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Construindo a cadeia RAG...\n",
            "✅ Cadeia RAG montada e pronta para uso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Faça uma pergunta relacionada aos PDFs que você subiu\n",
        "query = \"De acordo com o Código de Defesa do Consumidor, qual é o prazo para reclamar de vícios aparentes em produtos não duráveis e em produtos duráveis?\"\n",
        "\n",
        "print(f\"Enviando a pergunta: '{query}'\")\n",
        "print(\"\\nAguardando a resposta do sistema RAG...\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "# O método .invoke() executa a cadeia completa\n",
        "response = rag_chain.invoke(query)\n",
        "\n",
        "# Imprime a resposta final\n",
        "print(response)\n",
        "print(\"=\"*30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFq6i2YFK1hX",
        "outputId": "3011bb3d-3a38-4d24-becf-0adf57904502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enviando a pergunta: 'De acordo com o Código de Defesa do Consumidor, qual é o prazo para reclamar de vícios aparentes em produtos não duráveis e em produtos duráveis?'\n",
            "\n",
            "Aguardando a resposta do sistema RAG...\n",
            "==============================\n",
            "\n",
            "Você é um assistente jurídico no Brasil. Use APENAS o contexto a seguir para responder à pergunta.\n",
            "Se a resposta não estiver no contexto, diga \"Com base nos documentos fornecidos, não encontrei a informação.\"\n",
            "Cite a fonte do documento se possível.\n",
            "\n",
            "Contexto: Fonte: Consumidor.pdf\n",
            "§ 3° O fornecedor de serviços só não será responsabilizado quando provar:\n",
            "I - que, tendo prestado o serviço, o defeito inexiste;\n",
            "II - a culpa exclusiva do consumidor ou de terceiro.\n",
            "§ 4° A responsabilidade pessoal dos profissionais liberais será apurada mediante a verificação de culpa.\n",
            "Art. 15. (Vetado).\n",
            "Art. 16. (Vetado).\n",
            "Art. 17. Para os efeitos desta Seção, equiparam-se aos consumidores todas as vítimas do evento.\n",
            "Da Responsabilidade por Vício do Produto e do Serviço\n",
            "Art. 18. Os fornecedores de produtos de consumo duráveis ou não duráveis respondem solidariamente pelos vícios\n",
            "de qualidade ou quantidade que os tornem impróprios ou inadequados ao consumo a que se destinam ou lhes diminuam\n",
            "o valor, assim como por aqueles decorrentes da disparidade, com a indicações constantes do recipiente, da embalagem,\n",
            "rotulagem ou mensagem publicitária, respeitadas as variações decorrentes de sua natureza, podendo o consumidor\n",
            "exigir a substituição das partes viciadas.\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "§ 5° No caso de fornecimento de produtos in natura, será responsável perante o consumidor o fornecedor imediato,\n",
            "exceto quando identificado claramente seu produtor.\n",
            "§ 6° São impróprios ao uso e consumo:\n",
            "I - os produtos cujos prazos de validade estejam vencidos;\n",
            "II - os produtos deteriorados, alterados, adulterados, avariados, falsificados, corrompidos, fraudados, nocivos à vida\n",
            "ou à saúde, perigosos ou, ainda, aqueles em desacordo com as normas regulamentares de fabricação, distribuição ou\n",
            "apresentação;\n",
            "III - os produtos que, por qualquer motivo, se revelem inadequados ao fim a que se destinam.\n",
            "Art. 19. Os fornecedores respondem solidariamente pelos vícios de quantidade do produto sempre que, respeitadas\n",
            "as variações decorrentes de sua natureza, seu conteúdo líquido for inferior às indicações constantes do recipiente, da\n",
            "embalagem, rotulagem ou de mensagem publicitária, podendo o consumidor exigir, alternativamente e à sua escolha:\n",
            "I - o abatimento proporcional do preço;\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "implícita a obrigação do fornecedor de empregar componentes de reposição originais adequados e novos, ou que\n",
            "mantenham as especificações técnicas do fabricante, salvo, quanto a estes últimos, autorização em contrário do\n",
            "consumidor.\n",
            "https://www.planalto.gov.br/ccivil_03/leis/l8078compilado.htm 5/23\n",
            "30/09/2025, 10:33 L8078compilado\n",
            "Art. 22. Os órgãos públicos, por si ou suas empresas, concessionárias, permissionárias ou sob qualquer outra forma\n",
            "de empreendimento, são obrigados a fornecer serviços adequados, eficientes, seguros e, quanto aos essenciais,\n",
            "Parágrafo único. Nos casos de descumprimento, total ou parcial, das obrigações referidas neste artigo, serão as\n",
            "pessoas jurídicas compelidas a cumpri-las e a reparar os danos causados, na forma prevista neste código.\n",
            "Art. 23. A ignorância do fornecedor sobre os vícios de qualidade por inadequação dos produtos e serviços não o\n",
            "exime de responsabilidade.\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "comunicação com relação a produtos e serviços oferecidos ou apresentados, obriga o fornecedor que a fizer veicular ou\n",
            "dela se utilizar e integra o contrato que vier a ser celebrado.\n",
            "Art. 31. A oferta e apresentação de produtos ou serviços devem assegurar informações corretas, claras, precisas,\n",
            "ostensivas e em língua portuguesa sobre suas características, qualidades, quantidade, composição, preço, garantia,\n",
            "prazos de validade e origem, entre outros dados, bem como sobre os riscos que apresentam à saúde e segurança dos\n",
            "consumidores.\n",
            "Parágrafo único. As informações de que trata este artigo, nos produtos refrigerados oferecidos ao consumidor,\n",
            "serão gravadas de forma indelével. (Incluído pela Lei nº 11.989, de 2009)\n",
            "Art. 32. Os fabricantes e importadores deverão assegurar a oferta de componentes e peças de reposição enquanto\n",
            "não cessar a fabricação ou importação do produto.\n",
            "Parágrafo único. Cessadas a produção ou importação, a oferta deverá ser mantida por período razoável de tempo,\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 23. A ignorância do fornecedor sobre os vícios de qualidade por inadequação dos produtos e serviços não o\n",
            "exime de responsabilidade.\n",
            "Art. 24. A garantia legal de adequação do produto ou serviço independe de termo expresso, vedada a exoneração\n",
            "contratual do fornecedor.\n",
            "Art. 25. É vedada a estipulação contratual de cláusula que impossibilite, exonere ou atenue a obrigação de indenizar\n",
            "prevista nesta e nas seções anteriores.\n",
            "§ 1° Havendo mais de um responsável pela causação do dano, todos responderão solidariamente pela reparação\n",
            "prevista nesta e nas seções anteriores.\n",
            "§ 2° Sendo o dano causado por componente ou peça incorporada ao produto ou serviço, são responsáveis\n",
            "solidários seu fabricante, construtor ou importador e o que realizou a incorporação.\n",
            "Da Decadência e da Prescrição\n",
            "Art. 26. O direito de reclamar pelos vícios aparentes ou de fácil constatação caduca em:\n",
            "I - trinta dias, tratando-se de fornecimento de serviço e de produtos não duráveis;\n",
            "\n",
            "Pergunta: De acordo com o Código de Defesa do Consumidor, qual é o prazo para reclamar de vícios aparentes em produtos não duráveis e em produtos duráveis?\n",
            "\n",
            "Resposta:\n",
            "Com base nos documentos fornecidos, o prazo para reclamar de vícios aparentes em produtos não duráveis é de 30 dias, tratando-se de fornecimento de serviço e de produtos não duráveis. (Art. 26, I, Fonte: Consumidor.pdf) 30/09/2025, 10:33 L8078compilado\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "II - noventa dias, tratando-se de produtos duráveis.\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 27. O direito de reclamar pelos vícios não aparentes ou de difícil constatação caduca em:\n",
            "I - cento e oitenta dias, tratando-se de fornecimento de serviço e de produtos não duráveis;\n",
            "II - dois anos, tratando-se de produtos duráveis.\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 28. O prazo de decadência e prescrição começará a correr a partir da data do conhecimento do consumidor\n",
            "dos vícios ou da data do término do prazo de validade, se for aplicável.\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 29. O consumidor tem direito a reclamar do fornecedor, dentro do prazo de decadência, a reparação dos danos\n",
            "causados, desde que o vício seja aparente ou não aparente, e a exigir a substituição das partes viciadas.\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 30. A reclamação do consumidor pode ser feita por escrito, verbalmente ou por meio de comunicação eletrônica.\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 31. A oferta e apresentação de produtos ou serviços devem assegurar informações corretas, claras, precisas, ostensivas\n",
            "e em língua portuguesa sobre suas características, qualidades, quantidade, composição, preço, garantia, prazos de validade\n",
            "e origem, entre outros dados, bem como sobre os riscos que apresentam à saúde e segurança dos consumidores.\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 32. Os fabricantes e importadores deverão assegurar a oferta de componentes e peças de reposição enquanto\n",
            "não cessar a fabricação ou importação do produto.\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 33. A reclamação do consumidor pode ser feita por escrito, verbalmente ou por meio de comunicação eletrônica.\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 34. A reclamação do consumidor pode ser feita por escrito, verbalmente ou por meio de comunicação eletrônica.\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 35. A reclamação do consumidor pode ser feita por escrito, verbalmente ou por meio de comunicação eletrônica.\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 36. A reclamação do consumidor pode ser feita por escrito, verbalmente ou por meio de comunicação eletrônica.\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 37. A reclamação do consumidor pode ser feita por escrito, verbalmente ou por meio de comunicação eletrônica.\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 38. A reclamação do consumidor pode ser feita por escrito, verbalmente ou por meio de comunicação eletrônica.\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 39. A reclamação do consumidor pode ser feita por escrito, verbalmente ou por meio de comunicação eletrônica.\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 40. A reclamação do consumidor pode ser feita por escrito, verbalmente ou por meio de comunicação eletrônica.\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 41. A reclamação do consumidor pode ser feita por escrito, verbalmente ou por meio de comunicação eletrônica.\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 42. A reclamação do consumidor pode ser feita por escrito, verbalmente ou por meio de comunicação eletrônica.\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 43. A reclamação do consumidor pode ser feita por escrito, verbalmente ou por meio de comunicação\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Definindo a função de classificação...\")\n",
        "\n",
        "# Prompt para instruir o LLM a agir como um classificador\n",
        "CLASSIFIER_PROMPT = \"\"\"\n",
        "Sua única tarefa é classificar a pergunta do usuário em uma das 4 categorias abaixo. Retorne APENAS o nome da categoria.\n",
        "\n",
        "Categorias:\n",
        "1.  'rag_required': A pergunta exige conhecimento de leis, artigos, códigos ou jurisprudência.\n",
        "2.  'calculation_required': A pergunta pede um cálculo numérico baseado em regras jurídicas.\n",
        "3.  'general_conversation': É uma saudação, agradecimento ou conversa casual.\n",
        "4.  'out_of_scope': A pergunta não tem relação com o domínio jurídico.\n",
        "\n",
        "Pergunta do Usuário: \"{user_query}\"\n",
        "\n",
        "Categoria:\n",
        "\"\"\"\n",
        "\n",
        "# Criamos uma cadeia específica para a classificação\n",
        "classifier_chain = (\n",
        "    PromptTemplate.from_template(CLASSIFIER_PROMPT)\n",
        "    | llm_for_chain # Usamos o mesmo LLM já preparado\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "def classify_query(user_query: str) -> str:\n",
        "    \"\"\"Classifica a query do usuário em uma das 4 categorias.\"\"\"\n",
        "    response = classifier_chain.invoke({\"user_query\": user_query})\n",
        "    # Limpeza para garantir que apenas o nome da categoria seja retornado\n",
        "    clean_response = response.strip().lower()\n",
        "\n",
        "    valid_categories = [\"rag_required\", \"calculation_required\", \"general_conversation\", \"out_of_scope\"]\n",
        "    for category in valid_categories:\n",
        "        if category in clean_response:\n",
        "            print(f\"Query classificada como: '{category}'\")\n",
        "            return category\n",
        "\n",
        "    print(\"Classificação não foi clara, retornando 'out_of_scope'.\")\n",
        "    return \"out_of_scope\" # Fallback seguro\n",
        "\n",
        "print(\"✅ Função 'classify_query' pronta.\")\n",
        "\n",
        "# Teste rápido (opcional)\n",
        "# classify_query(\"Olá, tudo bem?\")\n",
        "# classify_query(\"Como calculo minhas férias?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZAJdBgwK2Bb",
        "outputId": "4dccff36-36be-4c16-fc1d-1e28ad58b96d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definindo a função de classificação...\n",
            "✅ Função 'classify_query' pronta.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# --- Disclaimers (Avisos Legais) ---\n",
        "DISCLAIMER_RAG = \"\\n\\n---\\n*AVISO: Esta resposta foi gerada por IA e não substitui a consulta a um profissional do Direito. Verifique as fontes.*\"\n",
        "DISCLAIMER_GENERAL = \"\\n\\n---\\n*AVISO: Esta é uma resposta conversacional e não deve ser interpretada como aconselhamento jurídico.*\"\n",
        "\n",
        "# --- A Função Mestra ---\n",
        "def chatbot_response(message, history):\n",
        "    \"\"\"\n",
        "    Função principal que o Gradio irá chamar.\n",
        "    Orquestra a classificação e a geração da resposta.\n",
        "    \"\"\"\n",
        "    # 1. Classifica a pergunta\n",
        "    classification = classify_query(message)\n",
        "\n",
        "    # 2. Age com base na classificação\n",
        "    if classification == 'rag_required' or classification == 'calculation_required':\n",
        "        # Se for RAG, chama a cadeia RAG que já testamos\n",
        "        response = rag_chain.invoke(message)\n",
        "        final_response = response.strip() + DISCLAIMER_RAG\n",
        "\n",
        "    elif classification == 'general_conversation':\n",
        "        # Se for conversa, usa o LLM com um prompt simples\n",
        "        general_prompt = f\"Você é um assistente virtual. Responda à seguinte mensagem de forma breve e amigável: '{message}'\"\n",
        "        response = llm_for_chain.invoke(general_prompt)\n",
        "        final_response = response.strip() + DISCLAIMER_GENERAL\n",
        "\n",
        "    else: # out_of_scope\n",
        "        # Se for fora de escopo, retorna uma mensagem padrão\n",
        "        final_response = \"Desculpe, meu conhecimento é focado em legislação brasileira. Não consigo ajudar com este tópico.\"\n",
        "\n",
        "    return final_response\n",
        "\n",
        "# --- 3. Inicia a Interface Gradio ---\n",
        "print(\"\\nIniciando a interface do Chatbot Jurídico...\")\n",
        "\n",
        "gr.ChatInterface(\n",
        "    fn=chatbot_response,\n",
        "    title=\"⚖️ Chatbot Jurídico Inteligente (Local)\",\n",
        "    description=\"Faça uma pergunta sobre a CLT, Código de Defesa do Consumidor ou Código Civil com base nos arquivos carregados.\",\n",
        "    examples=[\n",
        "        \"Qual o prazo para reclamar de vícios em produtos duráveis?\",\n",
        "        \"Como funciona a jornada de trabalho de 12 por 36 horas?\",\n",
        "        \"O que é considerado união estável?\",\n",
        "        \"Olá, qual seu nome?\"\n",
        "    ],\n",
        "    cache_examples=False\n",
        ").launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        },
        "id": "SzY5HNfnRH7V",
        "outputId": "951c5497-5b35-4f71-ef16-69f98732742f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Iniciando a interface do Chatbot Jurídico...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:348: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://db7804d9983044d510.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://db7804d9983044d510.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query classificada como: 'rag_required'\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query classificada como: 'rag_required'\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query classificada como: 'rag_required'\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://db7804d9983044d510.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"📊 Iniciando a avaliação do classificador...\")\n",
        "\n",
        "# Amostra do Dataset de Teste de Classificação\n",
        "classification_test_set = {\n",
        "    \"rag_required\": [\n",
        "        \"o que a clt fala sobre férias?\",\n",
        "        \"qual a punição para o crime de roubo segundo o código penal?\",\n",
        "        \"tenho direito a danos morais por cobrança indevida?\",\n",
        "        \"como funciona a usucapião de um imóvel?\",\n",
        "    ],\n",
        "    \"calculation_required\": [\n",
        "        \"como calculo minha rescisão com aviso prévio?\",\n",
        "        \"qual o valor da multa de 40% do FGTS para um saldo de R$ 12.000?\",\n",
        "        \"qual o valor de 1/3 de férias sobre um salário de 3000 reais?\",\n",
        "    ],\n",
        "    \"general_conversation\": [\n",
        "        \"oi tudo bem?\",\n",
        "        \"muito obrigado pela ajuda\",\n",
        "        \"quem te criou?\",\n",
        "        \"você é um robô?\",\n",
        "    ],\n",
        "    \"out_of_scope\": [\n",
        "        \"qual a previsão do tempo para amanhã?\",\n",
        "        \"me conte uma piada\",\n",
        "        \"qual o melhor time de futebol do brasil?\",\n",
        "        \"receita de bolo de cenoura\",\n",
        "    ]\n",
        "}\n",
        "\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Executa a classificação para cada item do dataset\n",
        "for label, queries in classification_test_set.items():\n",
        "    for query in queries:\n",
        "        true_labels.append(label)\n",
        "        predicted_category = classify_query(query)\n",
        "        predicted_labels.append(predicted_category)\n",
        "\n",
        "# Gera o relatório de classificação\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Relatório de Performance do Classificador\")\n",
        "print(\"=\"*50)\n",
        "report = classification_report(true_labels, predicted_labels)\n",
        "print(report)\n",
        "print(\"=\"*50)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "GIdz3Z6iasZ2",
        "outputId": "7b337463-5b8b-4079-dc64-f8c7e9e2ca11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Iniciando a avaliação do classificador...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query classificada como: 'rag_required'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query classificada como: 'rag_required'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query classificada como: 'rag_required'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query classificada como: 'rag_required'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query classificada como: 'rag_required'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query classificada como: 'rag_required'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query classificada como: 'rag_required'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query classificada como: 'rag_required'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query classificada como: 'rag_required'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query classificada como: 'rag_required'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query classificada como: 'rag_required'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query classificada como: 'rag_required'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query classificada como: 'rag_required'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3950136370.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mtrue_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mpredicted_category\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_category\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2147021889.py\u001b[0m in \u001b[0;36mclassify_query\u001b[0;34m(user_query)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclassify_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_query\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;34m\"\"\"Classifica a query do usuário em uma das 4 categorias.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"user_query\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_query\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Limpeza para garantir que apenas o nome da categoria seja retornado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mclean_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3243\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3244\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3245\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3246\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3247\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         return (\n\u001b[0;32m--> 390\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    391\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    788\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    998\u001b[0m                 )\n\u001b[1;32m    999\u001b[0m             ]\n\u001b[0;32m-> 1000\u001b[0;31m             return self._generate_helper(\n\u001b[0m\u001b[1;32m   1001\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m             output = (\n\u001b[0;32m--> 815\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    816\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_community/llms/huggingface_pipeline.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;31m# Process batch of prompts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             responses = self.pipeline(\n\u001b[0m\u001b[1;32m    286\u001b[0m                 \u001b[0mbatch_prompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mpipeline_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     def preprocess(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1446\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m                 )\n\u001b[0;32m-> 1448\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1372\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generation_config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2537\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mgeneration_mode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAMPLE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGREEDY_SEARCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2538\u001b[0m             \u001b[0;31m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2539\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2540\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2868\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2869\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2870\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2872\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 459\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m                         \u001b[0mmonkey_patched_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m         \u001b[0;31m# Restore original forward methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_forward\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmonkey_patched_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         hidden_states, _ = self.self_attn(\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1947\u001b[0m     \u001b[0;31m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1948\u001b[0m     \u001b[0;31m# https://github.com/pytorch/pytorch/pull/115074\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1949\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Module\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1950\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_parameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q nltk"
      ],
      "metadata": {
        "id": "24knI2XMfb_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "print(\"\\n\\n📊 Iniciando a avaliação do Sistema RAG...\")\n",
        "\n",
        "# Amostra do Dataset de Teste RAG com perguntas e respostas de referência\n",
        "rag_test_set = [\n",
        "    {\n",
        "        \"question\": \"Qual o prazo para reclamar de vícios em produtos duráveis?\",\n",
        "        \"reference_answer\": \"O direito de reclamar por vícios aparentes em produtos duráveis caduca em noventa dias.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Até quantas horas extras por dia um funcionário pode fazer segundo a CLT?\",\n",
        "        \"reference_answer\": \"A duração diária do trabalho poderá ser acrescida de horas extras, em número não excedente de duas.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"O que acontece se eu me demitir sem justa causa antes do fim do contrato com prazo determinado?\",\n",
        "        \"reference_answer\": \"Havendo termo estipulado, o empregado não se poderá desligar do contrato, sem justa causa, sob pena de ser obrigado a indenizar o empregador dos prejuízos que desse fato lhe resultarem.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "total_bleu_score = 0\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Avaliando Respostas do RAG\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for item in rag_test_set:\n",
        "    question = item[\"question\"]\n",
        "    reference = item[\"reference_answer\"].split() # A resposta de referência como lista de palavras\n",
        "\n",
        "    # Gera a resposta com nosso sistema RAG\n",
        "    generated_answer = rag_chain.invoke(question)\n",
        "    generated = generated_answer.split() # A resposta gerada como lista de palavras\n",
        "\n",
        "    # Calcula o score BLEU\n",
        "    bleu_score = sentence_bleu([reference], generated, weights=(0.5, 0.5)) # Usando bi-grams\n",
        "    total_bleu_score += bleu_score\n",
        "\n",
        "    print(f\"Pergunta: {question}\")\n",
        "    print(f\"  -> Resposta Esperada: {item['reference_answer']}\")\n",
        "    print(f\"  -> Resposta Gerada: {generated_answer.strip()}\")\n",
        "    print(f\"  -> Score BLEU: {bleu_score:.4f}\")\n",
        "    print(\"-\"*20)\n",
        "\n",
        "avg_bleu = total_bleu_score / len(rag_test_set)\n",
        "print(f\"\\nScore BLEU Médio: {avg_bleu:.4f}\")\n",
        "print(\"=\"*50)\n",
        "# Um score acima de 0.5-0.6 já é considerado muito bom. Guarde este resultado."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Thokz3_fipO",
        "outputId": "af643c3e-5429-4523-ab9c-0897f03bddb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "📊 Iniciando a avaliação do Sistema RAG...\n",
            "\n",
            "==================================================\n",
            "Avaliando Respostas do RAG\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pergunta: Qual o prazo para reclamar de vícios em produtos duráveis?\n",
            "  -> Resposta Esperada: O direito de reclamar por vícios aparentes em produtos duráveis caduca em noventa dias.\n",
            "  -> Resposta Gerada: Você é um assistente jurídico no Brasil. Use APENAS o contexto a seguir para responder à pergunta.\n",
            "Se a resposta não estiver no contexto, diga \"Com base nos documentos fornecidos, não encontrei a informação.\"\n",
            "Cite a fonte do documento de onde extraiu a resposta.\n",
            "\n",
            "Contexto: Fonte: Consumidor.pdf\n",
            "Art. 26. O direito de reclamar pelos vícios aparentes ou de fácil constatação caduca em:\n",
            "I - trinta dias, tratando-se de fornecimento de serviço e de produtos não duráveis;\n",
            "II - noventa dias, tratando-se de fornecimento de serviço e de produtos duráveis.\n",
            "§ 1° Inicia-se a contagem do prazo decadencial a partir da entrega efetiva do produto ou do término da execução\n",
            "dos serviços.\n",
            "§ 2° Obstam a decadência:\n",
            "I - a reclamação comprovadamente formulada pelo consumidor perante o fornecedor de produtos e serviços até a\n",
            "resposta negativa correspondente, que deve ser transmitida de forma inequívoca;\n",
            "II - (Vetado).\n",
            "III - a instauração de inquérito civil, até seu encerramento.\n",
            "§ 3° Tratando-se de vício oculto, o prazo decadencial inicia-se no momento em que ficar evidenciado o defeito.\n",
            "Art. 27. Prescreve em cinco anos a pretensão à reparação pelos danos causados por fato do produto ou do serviço\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "§ 5° No caso de fornecimento de produtos in natura, será responsável perante o consumidor o fornecedor imediato,\n",
            "exceto quando identificado claramente seu produtor.\n",
            "§ 6° São impróprios ao uso e consumo:\n",
            "I - os produtos cujos prazos de validade estejam vencidos;\n",
            "II - os produtos deteriorados, alterados, adulterados, avariados, falsificados, corrompidos, fraudados, nocivos à vida\n",
            "ou à saúde, perigosos ou, ainda, aqueles em desacordo com as normas regulamentares de fabricação, distribuição ou\n",
            "apresentação;\n",
            "III - os produtos que, por qualquer motivo, se revelem inadequados ao fim a que se destinam.\n",
            "Art. 19. Os fornecedores respondem solidariamente pelos vícios de quantidade do produto sempre que, respeitadas\n",
            "as variações decorrentes de sua natureza, seu conteúdo líquido for inferior às indicações constantes do recipiente, da\n",
            "embalagem, rotulagem ou de mensagem publicitária, podendo o consumidor exigir, alternativamente e à sua escolha:\n",
            "I - o abatimento proporcional do preço;\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "§ 3° O fornecedor de serviços só não será responsabilizado quando provar:\n",
            "I - que, tendo prestado o serviço, o defeito inexiste;\n",
            "II - a culpa exclusiva do consumidor ou de terceiro.\n",
            "§ 4° A responsabilidade pessoal dos profissionais liberais será apurada mediante a verificação de culpa.\n",
            "Art. 15. (Vetado).\n",
            "Art. 16. (Vetado).\n",
            "Art. 17. Para os efeitos desta Seção, equiparam-se aos consumidores todas as vítimas do evento.\n",
            "Da Responsabilidade por Vício do Produto e do Serviço\n",
            "Art. 18. Os fornecedores de produtos de consumo duráveis ou não duráveis respondem solidariamente pelos vícios\n",
            "de qualidade ou quantidade que os tornem impróprios ou inadequados ao consumo a que se destinam ou lhes diminuam\n",
            "o valor, assim como por aqueles decorrentes da disparidade, com a indicações constantes do recipiente, da embalagem,\n",
            "rotulagem ou mensagem publicitária, respeitadas as variações decorrentes de sua natureza, podendo o consumidor\n",
            "exigir a substituição das partes viciadas.\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 23. A ignorância do fornecedor sobre os vícios de qualidade por inadequação dos produtos e serviços não o\n",
            "exime de responsabilidade.\n",
            "Art. 24. A garantia legal de adequação do produto ou serviço independe de termo expresso, vedada a exoneração\n",
            "contratual do fornecedor.\n",
            "Art. 25. É vedada a estipulação contratual de cláusula que impossibilite, exonere ou atenue a obrigação de indenizar\n",
            "prevista nesta e nas seções anteriores.\n",
            "§ 1° Havendo mais de um responsável pela causação do dano, todos responderão solidariamente pela reparação\n",
            "prevista nesta e nas seções anteriores.\n",
            "§ 2° Sendo o dano causado por componente ou peça incorporada ao produto ou serviço, são responsáveis\n",
            "solidários seu fabricante, construtor ou importador e o que realizou a incorporação.\n",
            "Da Decadência e da Prescrição\n",
            "Art. 26. O direito de reclamar pelos vícios aparentes ou de fácil constatação caduca em:\n",
            "I - trinta dias, tratando-se de fornecimento de serviço e de produtos não duráveis;\n",
            "\n",
            "Fonte: Consumidor.pdf\n",
            "implícita a obrigação do fornecedor de empregar componentes de reposição originais adequados e novos, ou que\n",
            "mantenham as especificações técnicas do fabricante, salvo, quanto a estes últimos, autorização em contrário do\n",
            "consumidor.\n",
            "https://www.planalto.gov.br/ccivil_03/leis/l8078compilado.htm 5/23\n",
            "30/09/2025, 10:33 L8078compilado\n",
            "Art. 22. Os órgãos públicos, por si ou suas empresas, concessionárias, permissionárias ou sob qualquer outra forma\n",
            "de empreendimento, são obrigados a fornecer serviços adequados, eficientes, seguros e, quanto aos essenciais,\n",
            "Parágrafo único. Nos casos de descumprimento, total ou parcial, das obrigações referidas neste artigo, serão as\n",
            "pessoas jurídicas compelidas a cumpri-las e a reparar os danos causados, na forma prevista neste código.\n",
            "Art. 23. A ignorância do fornecedor sobre os vícios de qualidade por inadequação dos produtos e serviços não o\n",
            "exime de responsabilidade.\n",
            "\n",
            "Pergunta: Qual o prazo para reclamar de vícios em produtos duráveis?\n",
            "\n",
            "---\n",
            "Após a resposta, adicione em uma nova linha sua confiança na resposta de 0 a 100% no formato \"Confiança: XX%\". A confiança deve ser alta se o contexto responde diretamente à pergunta.\n",
            "\n",
            "Resposta:\n",
            "Tratando-se de fornecimento de serviço e de produtos duráveis, o prazo decadencial é de noventa dias, a partir da entrega efetiva do produto ou do término da execução dos serviços.\n",
            "\n",
            "Confiança: 100%\n",
            "Fonte: Art. 26, Consumidor.pdf\n",
            "https://www.planalto.gov.br/ccivil_03/leis/l8078compilado.htm\n",
            "L8078compilado\n",
            "5/23\n",
            "30/09/2025, 10:33\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 26. O direito de reclamar pelos vícios aparentes ou de fácil constatação caduca em:\n",
            "I - trinta dias, tratando-se de fornecimento de serviço e de produtos não duráveis;\n",
            "II - noventa dias, tratando-se de fornecimento de serviço e de produtos duráveis.\n",
            "§ 1° Inicia-se a contagem do prazo decadencial a partir da entrega efetiva do produto ou do término da execução dos serviços.\n",
            "§ 2° Obstam a decadência:\n",
            "I - a reclamação comprovadamente formulada pelo consumidor perante o fornecedor de produtos e serviços até a resposta\n",
            "negativa correspondente, que deve ser transmitida de forma inequívoca;\n",
            "II - (Vetado).\n",
            "III - a instauração de inquérito civil, até seu encerramento.\n",
            "§ 3° Tratando-se de vício oculto, o prazo decadencial inicia-se no momento em que ficar evidenciado o defeito. Fonte: Consumidor.pdf\n",
            "https://www.planalto.gov.br/ccivil_03/leis/l8078compilado.htm\n",
            "L8078compilado\n",
            "5/23\n",
            "30/09/2025, 10:33\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 26. O direito de reclamar pelos vícios aparentes ou de fácil constatação caduca em:\n",
            "I - trinta dias, tratando-se de fornecimento de serviço e de produtos não duráveis;\n",
            "II - noventa dias, tratando-se de fornecimento de serviço e de produtos duráveis.\n",
            "§ 1° Inicia-se a contagem do prazo decadencial a partir da entrega efetiva do produto ou do término da execução dos serviços.\n",
            "§ 2° Obstam a decadência:\n",
            "I - a reclamação comprovadamente formulada pelo consumidor perante o fornecedor de produtos e serviços até a resposta\n",
            "negativa correspondente, que deve ser transmitida de forma inequívoca;\n",
            "II - (Vetado).\n",
            "III - a instauração de inquérito civil, até seu encerramento.\n",
            "§ 3° Tratando-se de vício oculto, o prazo decadencial inicia-se no momento em que ficar evidenciado o defeito. Fonte: Consumidor.pdf\n",
            "https://www.planalto.gov.br/ccivil_03/leis/l8078compilado.htm\n",
            "L8078compilado\n",
            "5/23\n",
            "30/09/2025, 10:33\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 26. O direito de reclamar pelos vícios aparentes ou de fácil constatação caduca em:\n",
            "I - trinta dias, tratando-se de fornecimento de serviço e de produtos não duráveis;\n",
            "II - noventa dias, tratando-se de fornecimento de serviço e de produtos duráveis.\n",
            "§ 1° Inicia-se a contagem do prazo decadencial a partir da entrega efetiva do produto ou do término da execução dos serviços.\n",
            "§ 2° Obstam a decadência:\n",
            "I - a reclamação comprovadamente formulada pelo consumidor perante o fornecedor de produtos e serviços até a resposta\n",
            "negativa correspondente, que deve ser transmitida de forma inequívoca;\n",
            "II - (Vetado).\n",
            "III - a instauração de inquérito civil, até seu encerramento.\n",
            "§ 3° Tratando-se de vício oculto, o prazo decadencial inicia-se no momento em que ficar evidenciado o defeito. Fonte: Consumidor.pdf\n",
            "https://www.planalto.gov.br/ccivil_03/leis/l8078compilado.htm\n",
            "L8078compilado\n",
            "5/23\n",
            "30/09/2025, 10:33\n",
            "Fonte: Consumidor.pdf\n",
            "Art. 26. O direito de reclamar pelos vícios aparentes ou de fácil constatação caduca em:\n",
            "I - trinta dias, tratando-se de fornecimento de serviço e de produtos não duráveis;\n",
            "II - noventa dias, tratando-se de fornecimento de serviço e de produtos duráveis\n",
            "  -> Score BLEU: 0.0068\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pergunta: Até quantas horas extras por dia um funcionário pode fazer segundo a CLT?\n",
            "  -> Resposta Esperada: A duração diária do trabalho poderá ser acrescida de horas extras, em número não excedente de duas.\n",
            "  -> Resposta Gerada: Você é um assistente jurídico no Brasil. Use APENAS o contexto a seguir para responder à pergunta.\n",
            "Se a resposta não estiver no contexto, diga \"Com base nos documentos fornecidos, não encontrei a informação.\"\n",
            "Cite a fonte do documento de onde extraiu a resposta.\n",
            "\n",
            "Contexto: Fonte: trabalhista.pdf\n",
            "homens e mulheres, em particular as que se destinam a corrigir as distorções que afetam a formação profissional, o acesso ao emprego e as condições\n",
            "gerais de trabalho da mulher. (Incluído pela Lei nº 9.799, de 26.5.1999)\n",
            "Art. 374. A duração normal do trabalho diurno da mulher poderá ser no máximo elevada de mais duas horas, mediante contrato coletivo ou acordo\n",
            "firmado entre empregados e empregadores, observado o limite de quarenta e oito horas semanais.\n",
            "Parágrafo único. O acordo ou contrato coletivo de trabalho deverá ser homologado pela autoridade competente e do mesmo constará,\n",
            "obrigatoriamente, a importância do salário da hora suplementar, que será igual a da hora normal acrescida de uma percentagem adicional de 20 % (vinte\n",
            "por cento) no mínimo.\n",
            " Art. 374 - A duração normal diária do trabalho da mulher poderá ser no máximo elevada de 2 (duas) horas, independentemente de\n",
            "\n",
            "Fonte: trabalhista.pdf\n",
            "da responsabilidade de terceiros, os operários escalados perceberão o tempo que ficarem paralisados, na base dos salários vigentes, cabendo às\n",
            "administrações dos portos, se não forem elas as responsáveis, o direito de cobrar a quantia paga pela inatividade à entidade que motivar a paralisação.\n",
            "§ 4º Quando a quantidade de mercadorias a manipular por uma turma for tão pequena que não assegure, para cada um dos operários e\n",
            "empregados escalados, o provento do meio dia de salário, ao menos, os operários e empregados perceberão a remuneração correspondente ao meio dia\n",
            "de salário vigente.\n",
            "§ 5º Se o trabalho a que se refere o parágrafo anterior exceder em duração a meio dia de trabalho e, em quantidade, a 30 toneladas, os operários\n",
            "perceberão a remuneração por salário, correspondente ao número de horas da efetiva duração do serviço.\n",
            "§ 6º Os operários mensalistas e os diaristas que, à data do decreto-lei nº 3.844, de 20 de novembro de 1941, tinham direito a determinada\n",
            "\n",
            "Fonte: trabalhista.pdf\n",
            "turma empregada na execução do serviço, distinguidos os casos de trabalhar um ou mais guindastes, por porão de navio, ou uma ou mais portas de\n",
            "Parágrafo único. Quando condições especias do serviço exigirem o aumento do número de trabalhadores fixados para compor as turmas, este\n",
            "aumento será feito, a critério das administrações dos portos, e a sua remuneração será idêntica à que couber aos trabalhadores componentes normais\n",
            "das turmas. (Revogado pela Lei nº 8.630, de 25.2.1993)\n",
            " Art. 288 - As taxas aprovadas para retribuir a mão de obra serão aplicadas à quantidade de mercadorias movimentada por cada turma e o\n",
            "produto será dividido na razão de uma quota para cada trabalhador, uma para cada motorista interno do armazém, uma e meia para o feitor, uma e um\n",
            "quarto para o ajudante do feitor, uma e meia para cada motorista do guindaste do cais, uma e meia para cada conferente. (Revogado pela Lei nº\n",
            "8.630, de 25.2.1993)\n",
            "\n",
            "Fonte: trabalhista.pdf\n",
            "c) quando o brasileiro for aprendiz, ajudante ou servente, e não o for o estrangeiro;\n",
            "d) quando a remuneração resultar de maior produção, para os que trabalham à comissão ou por tarefa.\n",
            "Parágrafo único - Nos casos de falta ou cessação de serviço, a dispensa do empregado estrangeiro deve preceder à de brasileiro que exerça\n",
            "função análoga.\n",
            "DAS RELAÇÕES ANUAIS DE EMPREGADOS\n",
            " Art. 359 - Nenhuma empresa poderá admitir a seu serviço empregado estrangeiro sem que este exiba a carteira de identidade de\n",
            "estrangeiro devidamente anotada .\n",
            "Parágrafo único - A empresa é obrigada a assentar no registro de empregados os dados referentes à nacionalidade de qualquer empregado\n",
            "estrangeiro e o número da respectiva carteira de identidade.\n",
            "Art. 360 - Toda empresa compreendida na enumeração do art. 352, § 1º, deste Capítulo, qualquer que seja o número de seus empregados, deve\n",
            "\n",
            "Fonte: trabalhista.pdf\n",
            "das tabelas aprovadas, com um acréscimo de 20% para cada hora suplementar.\n",
            "§ 2º Para ultimar a carga ou descarga dos grandes paquetes ou dos navios que estejam na iminência de perder a maré, e para não interromper o\n",
            "trabalho dos navios frigoríficos, o concessionário do porto poderá executar o serviço de capatazias durantes as horas destinadas às refeições dos\n",
            "operários, pagando-lhes, porém, como suplemento de remuneração, o dobro do salário correspondente à duração da refeição.\n",
            "§ 3º O trabalho à noite e aos domingos e feriados será considerado extraordinário e, como tal, pago com um acréscimo de 25% sobre o salário\n",
            " Art. 292 - As taxas de capatazias serão da responsabilidade dos donos das mercadorias, os dispêndios extraordinários, porém, que por\n",
            "esse serviço pagar o concessionário do porto na forma do § 2º do art. 288, e do § 2º do art. 291 serão debitados aos armadores que houverem requisitado\n",
            "\n",
            "Pergunta: Até quantas horas extras por dia um funcionário pode fazer segundo a CLT?\n",
            "\n",
            "---\n",
            "Após a resposta, adicione em uma nova linha sua confiança na resposta de 0 a 100% no formato \"Confiança: XX%\". A confiança deve ser alta se o contexto responde diretamente à pergunta.\n",
            "\n",
            "Resposta:\n",
            "A duração normal diária do trabalho da mulher poderá ser no máximo elevada de 2 (duas) horas, independentemente de...\n",
            "\n",
            "Confiança: 100%\n",
            "\n",
            "\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Fonte: trabalhista.pdf\n",
            " Font\n",
            "  -> Score BLEU: 0.0081\n",
            "--------------------\n",
            "Pergunta: O que acontece se eu me demitir sem justa causa antes do fim do contrato com prazo determinado?\n",
            "  -> Resposta Esperada: Havendo termo estipulado, o empregado não se poderá desligar do contrato, sem justa causa, sob pena de ser obrigado a indenizar o empregador dos prejuízos que desse fato lhe resultarem.\n",
            "  -> Resposta Gerada: Você é um assistente jurídico no Brasil. Use APENAS o contexto a seguir para responder à pergunta.\n",
            "Se a resposta não estiver no contexto, diga \"Com base nos documentos fornecidos, não encontrei a informação.\"\n",
            "Cite a fonte do documento de onde extraiu a resposta.\n",
            "\n",
            "Contexto: Fonte: trabalhista.pdf\n",
            "reconsiderar o ato, antes de seu termo, à outra parte é facultado aceitar ou não a reconsideração.\n",
            "Parágrafo único - Caso seja aceita a reconsideração ou continuando a prestação depois de expirado o prazo, o contrato continuará a vigorar,\n",
            "como se o aviso prévio não tivesse sido dado.\n",
            " Art. 490 - O empregador que, durante o prazo do aviso prévio dado ao empregado, praticar ato que justifique a rescisão imediata do\n",
            "contrato, sujeita-se ao pagamento da remuneração correspondente ao prazo do referido aviso, sem prejuízo da indenização que for devida.\n",
            " Art. 491 - O empregado que, durante o prazo do aviso prévio, cometer qualquer das faltas consideradas pela lei como justas para a\n",
            "rescisão, perde o direito ao restante do respectivo prazo.\n",
            "CAPÍTULO VII\n",
            "DA ESTABILIDADE\n",
            " Art. 492 - O empregado que contar mais de 10 (dez) anos de serviço na mesma empresa não poderá ser despedido senão por motivo\n",
            "de falta grave ou circunstância de força maior, devidamente comprovadas.\n",
            "\n",
            "Fonte: trabalhista.pdf\n",
            "exercício das suas funções, nem transferida sem causa justificada, a juizo do Ministério do Trabalho, Indústria e Comércio, para lugar ou mister que lhe\n",
            "dificulte ou torne impossivel o desempenho da comissão ou do mandato.\n",
            "§ 1º O empregado perderá o mandato se a transferência for por ele solicitada, ou voluntariamente aceita.\n",
            "§ 2º Considera-se de licença não remunerada, salvo assentimento do empregador ou cláusula contratual, o tempo em que o empregado se ausentar\n",
            "do trabalho no desempenho das funções a que se refere este artigo.\n",
            "§ 3º O empregador que despedir, suspender ou rebaixar de categoria o empregado, ou lhe reduzir o salário, para impedir que o mesmo se associe a\n",
            "sindicato, organize associação sindical ou exerça os direitos inerentes à condição de sindicalizado fica sujeito à penalidade prevista na alínea a, do artigo\n",
            "553, sem prejuizo da reparação a que tiver direito o empregado.\n",
            "\n",
            "Fonte: civil.pdf\n",
            "Parágrafo único. Se se despedir sem justa causa, terá direito à retribuição vencida, mas responderá por perdas e danos. O\n",
            "mesmo dar-se-á, se despedido por justa causa.\n",
            "Art. 603. Se o prestador de serviço for despedido sem justa causa, a outra parte será obrigada a pagar-lhe por inteiro a\n",
            "retribuição vencida, e por metade a que lhe tocaria de então ao termo legal do contrato.\n",
            "Art. 604. Findo o contrato, o prestador de serviço tem direito a exigir da outra parte a declaração de que o contrato está findo.\n",
            "Igual direito lhe cabe, se for despedido sem justa causa, ou se tiver havido motivo justo para deixar o serviço.\n",
            "Art. 605. Nem aquele a quem os serviços são prestados, poderá transferir a outrem o direito aos serviços ajustados, nem o\n",
            "prestador de serviços, sem aprazimento da outra parte, dar substituto que os preste.\n",
            "Art. 606. Se o serviço for prestado por quem não possua título de habilitação, ou não satisfaça requisitos outros estabelecidos\n",
            "\n",
            "Fonte: trabalhista.pdf\n",
            " Art. 487 - Não havendo prazo estipulado, a parte que, sem justo motivo, quiser rescindir o contrato deverá avisar a outra da sua\n",
            "resolução com a antecedência mínima de:\n",
            "I - 3 dias, se o empregado receber, diariamente, o seu salário;\n",
            "I - oito dias, se o pagamento for efetuado por semana ou tempo inferior; (Redação dada pela Lei nº 1.530, de 26.12.1951)\n",
            "II - 8 dias, se o pagamento for efetuado por semana ou tempo inferior;\n",
            "II - trinta dias aos que perceberem por quinzena ou mês, ou que tenham mais de 12 (doze) meses de serviço na empresa. (Redação dada\n",
            "pela Lei nº 1.530, de 26.12.1951)\n",
            "§ 1º - A falta do aviso prévio por parte do empregador dá ao empregado o direito aos salários correspondentes ao prazo do aviso, garantida\n",
            "sempre a integração desse período no seu tempo de serviço.\n",
            "§ 2º - A falta de aviso prévio por parte do empregado dá ao empregador o direito de descontar os salários correspondentes ao prazo respectivo.\n",
            "\n",
            "Fonte: civil.pdf\n",
            "sua zona, ainda que sem a sua interferência.\n",
            "Art. 715. O agente ou distribuidor tem direito à indenização se o proponente, sem justa causa, cessar o atendimento das\n",
            "propostas ou reduzi-lo tanto que se torna antieconômica a continuação do contrato.\n",
            "Art. 716. A remuneração será devida ao agente também quando o negócio deixar de ser realizado por fato imputável ao\n",
            "proponente.\n",
            "Art. 717. Ainda que dispensado por justa causa, terá o agente direito a ser remunerado pelos serviços úteis prestados ao\n",
            "proponente, sem embargo de haver este perdas e danos pelos prejuízos sofridos.\n",
            "Art. 718. Se a dispensa se der sem culpa do agente, terá ele direito à remuneração até então devida, inclusive sobre os\n",
            "negócios pendentes, além das indenizações previstas em lei especial.\n",
            "Art. 719. Se o agente não puder continuar o trabalho por motivo de força maior, terá direito à remuneração correspondente\n",
            "aos serviços realizados, cabendo esse direito aos herdeiros no caso de morte.\n",
            "\n",
            "Pergunta: O que acontece se eu me demitir sem justa causa antes do fim do contrato com prazo determinado?\n",
            "\n",
            "---\n",
            "Após a resposta, adicione em uma nova linha sua confiança na resposta de 0 a 100% no formato \"Confiança: XX%\". A confiança deve ser alta se o contexto responde diretamente à pergunta.\n",
            "\n",
            "Resposta:\n",
            "Segundo o Art. 490, se o empregador praticar ato que justifique a rescisão imediata do contrato durante o prazo do aviso prévio, sujeita-se ao pagamento da remuneração correspondente ao prazo do referido aviso, sem prejuízo da indenização que for devida.\n",
            "\n",
            "Se o empregado se demitir sem justa causa antes do fim do contrato com prazo determinado, não há a possibilidade de reconsideração do ato, pois a demissão é um ato unilateral do empregado. Nesse caso, o empregado não terá direito ao restante do prazo do contrato, pois o contrato se finda com a demissão.\n",
            "\n",
            "Confiança: 90%\n",
            "\n",
            "\n",
            "Com base nos documentos fornecidos, não encontrei a informação.\n",
            "Fonte: trabalhista.pdf\n",
            "Art. 490 - O empregador que, durante o prazo do aviso prévio dado ao empregado, praticar ato que justifique a rescisão imediata do\n",
            "contrato, sujeita-se ao pagamento da remuneração correspondente ao prazo do referido aviso, sem prejuízo da indenização que for devida.\n",
            "Fonte: trabalhista.pdf\n",
            "Art. 491 - O empregado que, durante o prazo do aviso prévio, cometer qualquer das faltas consideradas pela lei como justas para a\n",
            "rescisão, perde o direito ao restante do respectivo prazo. Fonte: trabalhista.pdf\n",
            "CAPÍTULO VII\n",
            "DA ESTABILIDADE\n",
            "Fonte: trabalhista.pdf\n",
            "exercício das suas funções, nem transferida sem causa justificada, a juizo do Ministério do Trabalho, Indústria e Comércio, para lugar ou mister que lhe\n",
            "dificulte ou torne impossivel o desempenho da comissão ou do mandato. Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "Fonte: trabalhista.pdf\n",
            "  -> Score BLEU: 0.0097\n",
            "--------------------\n",
            "\n",
            "Score BLEU Médio: 0.0082\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#\n",
        "Projeto Final: Chatbot Jurídico Inteligente com Classificador RAG\n",
        "\n",
        "**Autor:** Enzo Guimarães Miguel\n",
        "**Curso:** Pós-Graduação em IA, Modulo de NLP\n",
        "**Professor:** Dimmy Magalhães\n",
        "**Data:** 03 de Outubro de 2025\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Objetivo do Projeto\n",
        "\n",
        "O objetivo deste projeto foi desenvolver um protótipo completo de um chatbot jurídico inteligente. O sistema é capaz de receber perguntas em linguagem natural, classificar a intenção do usuário e, para questões jurídicas, utilizar a técnica de Geração Aumentada por Recuperação (RAG) para encontrar respostas em uma base de conhecimento personalizada e formular uma resposta fundamentada.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Arquitetura e Ferramentas Utilizadas\n",
        "\n",
        "A solução foi desenvolvida integralmente em um ambiente Google Colab com acesso a uma GPU T4. A escolha por uma execução local (no Colab) em detrimento de APIs externas foi feita para cumprir os objetivos acadêmicos do curso, demonstrando o gerenciamento completo do pipeline de modelos.\n",
        "\n",
        "As principais tecnologias utilizadas foram:\n",
        "\n",
        "* **Linguagem:** Python 3\n",
        "* **Modelo de Linguagem (LLM):** `meta-llama/Meta-Llama-3-8B-Instruct` (quantizado em 4-bit) para classificação de intenção e geração de respostas.\n",
        "* **Modelo de Embedding:** `rufimelo/Legal-BERTimbau-large` para a vetorização semântica dos textos jurídicos.\n",
        "* **Banco de Dados Vetorial:** `FAISS (Facebook AI Similarity Search)` para armazenamento e busca eficiente dos vetores de texto.\n",
        "* **Orquestração do Pipeline:** Framework `LangChain` para construir e gerenciar as cadeias de classificação e RAG.\n",
        "* **Interface do Usuário (UI):** Biblioteca `Gradio` para criar a interface de chat interativa.\n",
        "* **Processamento de Dados:** `PDFPlumber` para extração de texto dos documentos e `NLTK` para avaliação de métricas.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Metodologia e Pipeline\n",
        "\n",
        "O sistema funciona seguindo um pipeline bem definido, integrando as diferentes fases do projeto:\n",
        "\n",
        "1.  **Ingestão de Dados:** Documentos PDF (CLT, Código Civil, CDC) são carregados, limpos de caracteres indesejados e divididos em `chunks` (pedaços) de 1000 caracteres com 200 de sobreposição, utilizando a arquitetura base fornecida.\n",
        "\n",
        "2.  **Vetorização:** Cada `chunk` é processado pelo modelo `Legal-BERTimbau-large` e transformado em um vetor numérico (embedding).\n",
        "\n",
        "3.  **Indexação:** Todos os vetores são armazenados e indexados em um `vector store` utilizando FAISS, o que permite buscas de similaridade semântica de alta velocidade.\n",
        "\n",
        "4.  **Interface e Interação:** Uma interface do Gradio recebe a pergunta do usuário.\n",
        "\n",
        "5.  **Classificação de Intenção:** A pergunta é primeiro enviada para uma cadeia de classificação que usa o Llama 3 para determinar se a pergunta é `rag_required`, `general_conversation` ou `out_of_scope`.\n",
        "\n",
        "6.  **Recuperação e Geração (RAG):** Se classificada como `rag_required`, a pergunta é usada para buscar os 5 `chunks` mais relevantes no índice FAISS. Esses `chunks` formam o `contexto`, que é então injetado em um prompt junto com a pergunta original e enviado ao Llama 3 para formular a resposta final.\n",
        "\n",
        "7.  **Resposta ao Usuário:** A resposta gerada, junto com as fontes, nível de confiança e disclaimers, é exibida na interface.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Resultados da Avaliação (Fase 5)\n",
        "\n",
        "O desempenho do sistema foi medido com scripts automatizados para avaliar separadamente o Classificador de Intenção e o pipeline de RAG (Recuperação e Geração).\n",
        "\n",
        "### 4.1. Desempenho do Classificador\n",
        "\n",
        "O classificador de intenção, responsável por direcionar a pergunta do usuário, foi avaliado com um dataset representativo. Os resultados de Precisão (precision), Revocação (recall) e F1-Score (f1-score) foram os seguintes:\n",
        "\n",
        "| Categoria              | Precision | Recall | F1-Score | Suporte (amostras) |\n",
        "|:-----------------------|:---------:|:------:|:--------:|:------------------:|\n",
        "| `calculation_required` |   1.00    |  1.00  |   1.00   |         3          |\n",
        "| `general_conversation` |   1.00    |  1.00  |   1.00   |         4          |\n",
        "| `out_of_scope`         |   1.00    |  1.00  |   1.00   |         4          |\n",
        "| `rag_required`         |   1.00    |  1.00  |   1.00   |         4          |\n",
        "|                        |           |        |          |                    |\n",
        "| **Acurácia** |           |        | **1.00** |     **15** |\n",
        "| **Média Macro** |   1.00    |  1.00  |   1.00   |         15         |\n",
        "| **Média Ponderada** |   1.00    |  1.00  |   1.00   |         15         |\n",
        "\n",
        "\n",
        "\n",
        "**Análise do Classificador:** O classificador obteve um desempenho perfeito (100% em todas as métricas) na amostra de teste. Isso demonstra que a abordagem de usar o LLM `Llama 3` com um prompt de poucos exemplos (*few-shot*) é extremamente eficaz para distinguir as diferentes intenções do usuário, garantindo que a ferramenta correta (RAG, conversa geral, etc.) seja acionada para cada caso.\n",
        "\n",
        "### 4.2. Qualidade do Sistema RAG\n",
        "\n",
        "A qualidade da geração de respostas foi avaliada com 3 perguntas-chave e suas respectivas respostas de referência. A análise combina uma avaliação qualitativa (o retriever encontrou o contexto correto?) e quantitativa (o score BLEU, que mede a similaridade textual).\n",
        "\n",
        "Avaliando Respostas do RAG\n",
        "Pergunta: Qual o prazo para reclamar de vícios em produtos duráveis? -> Resposta Esperada: O direito de reclamar por vícios aparentes em produtos duráveis caduca em noventa dias. -> Resposta Gerada: Tratando-se de fornecimento de serviço e de produtos duráveis, o prazo decadencial é de noventa dias, a partir da entrega efetiva do produto ou do término da execução dos serviços. -> Score BLEU: 0.0068\n",
        "Pergunta: Até quantas horas extras por dia um funcionário pode fazer segundo a CLT? -> Resposta Esperada: A duração diária do trabalho poderá ser acrescida de horas extras, em número não excedente de duas. -> Resposta Gerada: A duração normal diária do trabalho da mulher poderá ser no máximo elevada de 2 (duas) horas, independentemente de... -> Score BLEU: 0.0081\n",
        "Pergunta: O que acontece se eu me demitir sem justa causa antes do fim do contrato com prazo determinado? -> Resposta Esperada: Havendo termo estipulado, o empregado não se poderá desligar do contrato, sem justa causa, sob pena de ser obrigado a indenizar o empregador dos prejuízos que desse fato lhe resultarem. -> Resposta Gerada: Segundo o Art. 490, se o empregador praticar ato que justifique a rescisão imediata do contrato durante o prazo do aviso prévio, sujeita-se ao pagamento da remuneração correspondente... -> Score BLEU: 0.0097\n",
        "Score BLEU Médio: 0.0082\n",
        "\n",
        "\n",
        "**Análise do RAG:** Os testes revelam um ponto central sobre o desempenho do sistema.\n",
        "* **Caso de Sucesso (Pergunta 1):** O sistema respondeu corretamente sobre o prazo para produtos duráveis. A análise do log mostra que o retriever foi bem-sucedido em encontrar o **Art. 26 do CDC**, fornecendo o contexto exato para o LLM gerar a resposta.\n",
        "* **Casos de Falha (Perguntas 2 e 3):** Para as perguntas sobre a CLT, o retriever não encontrou os artigos corretos (Art. 59 para horas extras e Art. 480 para rescisão antecipada). Ele recuperou artigos próximos semanticamente, mas incorretos para a pergunta específica. Com o contexto errado, o LLM não conseguiu responder corretamente, o que demonstra a dependência crítica da qualidade da etapa de recuperação.\n",
        "* **Score BLEU:** A métrica BLEU resultou em valores baixos, o que é esperado, pois ela mede a sobreposição exata de palavras e sentenças. Mesmo na resposta correta, a redação do LLM foi diferente da resposta de referência, o que penaliza o score. Isso indica que, para este caso, a **avaliação qualitativa (verificar se a informação está correta) é mais importante que a pontuação BLEU.**\n",
        "\n",
        "\n",
        "## 5. Análise de Casos Extremos\n",
        "\n",
        "Para avaliar a robustez e as limitações do chatbot, foram realizados testes qualitativos com perguntas fora do padrão (\"casos extremos\"). O objetivo foi observar o comportamento do sistema diante de ambiguidades, perguntas fora de escopo e diferentes níveis de complexidade.\n",
        "\n",
        "* **Perguntas Vagas ou Abertas:** Ao receber uma pergunta ambígua como *\"fale sobre direitos do trabalhador\"*, o sistema de recuperação (retriever) tendeu a buscar os trechos mais genéricos dos documentos, como os artigos iniciais da CLT. Consequentemente, o LLM gerou uma resposta ampla e pouco específica. Isso demonstra que a eficácia do RAG é diretamente proporcional à especificidade da pergunta do usuário, destacando a importância de formular questões claras para obter respostas detalhadas.\n",
        "\n",
        "* **Perguntas Fora de Escopo:** Em testes com perguntas claramente fora do domínio jurídico (ex: *\"Qual a previsão do tempo para Teresina?\"* ou *\"Quem vai ganhar o campeonato brasileiro?\"*), o sistema se comportou de forma ideal. O classificador de intenção, com alta precisão, identificou corretamente as perguntas como `out_of_scope` e acionou a resposta padrão, informando sua limitação de conhecimento. Isso valida o classificador como um \"gatekeeper\" eficaz, prevenindo o uso desnecessário e incorreto do pipeline RAG.\n",
        "\n",
        "* **Sensibilidade do Retriever:** A análise revelou que o ponto mais sensível do sistema é a etapa de recuperação de informação. Em perguntas muito específicas cuja resposta estava contida em um único artigo (como a questão sobre os prazos no CDC), o sistema performou bem. No entanto, em perguntas onde os termos-chave eram mais comuns e espalhados pelo documento (como as questões sobre a CLT), o retriever por vezes falhou em localizar o `chunk` mais relevante, resultando em respostas incorretas, como observado na avaliação da Fase 5.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Conclusão\n",
        "\n",
        "Este projeto demonstrou com sucesso a construção de um chatbot jurídico de ponta a ponta, integrando componentes de classificação de intenção, recuperação de informação e geração de linguagem natural. A arquitetura, baseada na biblioteca LangChain e utilizando modelos open-source (`Llama 3`, `Legal-BERTimbau`), provou-se funcional e robusta dentro do ambiente controlado do Google Colab com GPU.\n",
        "\n",
        "Os resultados da avaliação quantitativa foram mistos e geraram aprendizados importantes. Enquanto o classificador de intenção alcançou performance perfeita na amostra de teste, o sistema RAG, apesar de funcional, mostrou uma forte dependência da qualidade do retriever. As falhas em recuperar o contexto correto para certas perguntas da CLT evidenciam que a busca puramente semântica pode ser um ponto de fragilidade em documentos densos e complexos, mesmo com o aumento do número de chunks recuperados (`k=5`). O LLM, por sua vez, demonstrou alta capacidade de seguir instruções, respondendo corretamente quando o contexto era preciso e se recusando a responder quando não era.\n",
        "\n",
        "Conclui-se que o protótipo é um sucesso, não apenas por funcionar, mas por permitir a identificação clara de seus pontos fortes e fracos. Ele serve como uma excelente base para futuras melhorias, que poderiam transformar esta prova de conceito em uma ferramenta ainda mais poderosa e precisa.\n",
        "\n",
        "### Melhorias Futuras\n",
        "\n",
        "* **Implementação de Busca Híbrida:** Combinar a busca semântica (FAISS) com uma busca por palavra-chave (como BM25) para criar um `EnsembleRetriever`. Isso mitigaria as falhas observadas, pois termos exatos como \"horas extras\" ou \"rescisão de contrato\" seriam capturados com maior precisão.\n",
        "* **Otimização da Estratégia de *Chunking*:** Estudar diferentes tamanhos de `chunk` e `overlap` para encontrar um balanço ideal que evite que artigos de lei importantes sejam divididos de forma a perder seu significado semântico para o retriever.\n",
        "* **Refinamento do Prompt de Geração:** Aprimorar ainda mais o prompt do RAG para que o LLM consiga extrair e sintetizar informações de múltiplos `chunks` de forma mais eficaz, mesmo que alguns deles sejam parcialmente relevantes.\n",
        "* **Implantação (Deploy):** Como passo final de um ciclo de desenvolvimento, o aplicativo Gradio poderia ser implantado em uma plataforma como o Hugging Face Spaces para criar uma demonstração pública e funcional."
      ],
      "metadata": {
        "id": "57MGJYmSg20H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2wgXdfrUlM3A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}